{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"basic_audio.csv\")\n",
    "Y = data.year;\n",
    "X = data.drop('year', axis = 1)\n",
    "X = X.drop('idx', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7293 [00:00<?, ?it/s]/home/brandon/python3_environments/ds19_project/lib/python3.6/site-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "100%|██████████| 7293/7293 [02:56<00:00, 41.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"Floor\" the years\n",
    "for item in tqdm(range(0, len(Y))):\n",
    "    Y.iloc[item] = int(Y.iloc[item] / 10) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 100\n",
    "np.random.seed(random_state)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.2, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_range = [.01]\n",
    "alpha_range = np.arange(.1, .9, .1)\n",
    "lamda_range = np.arange(.1, .5, .1)\n",
    "estimators_range = [25, 50]\n",
    "max_depth_range = [20, 30]\n",
    "\n",
    "params = dict(reg_alpha = alpha_range,\n",
    "    reg_lambda = lamda_range,\n",
    "    max_depth = max_depth_range,\n",
    "    learning_rate = learning_range,\n",
    "    n_estimators = estimators_range,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=6)]: Done 438 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=6)]: Done 640 out of 640 | elapsed:  6.6min finished\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(random_state = random_state)\n",
    "grid_search = GridSearchCV(model, params, cv=5, verbose=1, n_jobs = 6)\n",
    "grid_search.fit(x_train, y_train)\n",
    "output = grid_search.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'max_depth': 20, 'n_estimators': 50, 'reg_alpha': 0.5, 'reg_lambda': 0.4}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = grid_search.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9280328992460589"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = grid_search.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1990. 1960. 2000. ... 2000. 1990. 2000.]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9635266694090223"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.495269436445906"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(Y, output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prob = grid_search.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_year = pd.DataFrame(output, columns = ['year_predict'])\n",
    "prob_df = pd.DataFrame(output_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_output = prob_df.join(output_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6  \\\n",
      "0     0.073057  0.073096  0.073097  0.073176  0.073114  0.073381  0.073323   \n",
      "1     0.085356  0.085402  0.085403  0.085496  0.224500  0.085735  0.085667   \n",
      "2     0.073113  0.073152  0.073153  0.073232  0.073170  0.073437  0.073379   \n",
      "3     0.071891  0.071929  0.071930  0.072009  0.071947  0.090648  0.331908   \n",
      "4     0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "5     0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "6     0.073241  0.073281  0.073281  0.073361  0.073299  0.073567  0.338143   \n",
      "7     0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "8     0.072487  0.072526  0.072527  0.072606  0.072544  0.074150  0.072751   \n",
      "9     0.072009  0.072047  0.072048  0.072126  0.072065  0.072328  0.073521   \n",
      "10    0.073268  0.073307  0.100961  0.073387  0.073325  0.075916  0.290845   \n",
      "11    0.073254  0.073293  0.073294  0.073374  0.073311  0.073579  0.338200   \n",
      "12    0.073058  0.073097  0.073098  0.073178  0.073115  0.073383  0.337297   \n",
      "13    0.073232  0.073271  0.073272  0.073352  0.073289  0.073557  0.073499   \n",
      "14    0.075405  0.075446  0.075447  0.241904  0.084630  0.075740  0.075680   \n",
      "15    0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "16    0.073861  0.073901  0.073902  0.073982  0.073919  0.074189  0.074130   \n",
      "17    0.073136  0.073175  0.073175  0.073255  0.073193  0.073460  0.337654   \n",
      "18    0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "19    0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "20    0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "21    0.073057  0.073096  0.073097  0.073176  0.073114  0.073381  0.073323   \n",
      "22    0.072763  0.072801  0.072802  0.072882  0.333443  0.073086  0.073027   \n",
      "23    0.073008  0.073047  0.073048  0.073127  0.073065  0.338868  0.073274   \n",
      "24    0.074455  0.074494  0.074495  0.074576  0.074513  0.074785  0.074725   \n",
      "25    0.073422  0.073462  0.073463  0.073542  0.336467  0.073748  0.073690   \n",
      "26    0.073057  0.073096  0.073097  0.073176  0.073114  0.073381  0.073323   \n",
      "27    0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "28    0.073048  0.073087  0.073088  0.073167  0.073105  0.073372  0.073314   \n",
      "29    0.073057  0.073096  0.073097  0.073176  0.073114  0.073381  0.073323   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "7263  0.079212  0.079254  0.079255  0.079341  0.079273  0.079563  0.246109   \n",
      "7264  0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "7265  0.073057  0.073096  0.073097  0.073176  0.073114  0.073381  0.073323   \n",
      "7266  0.074532  0.074571  0.074572  0.074653  0.074590  0.074862  0.076097   \n",
      "7267  0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "7268  0.073048  0.073087  0.073088  0.073167  0.073105  0.073372  0.073314   \n",
      "7269  0.073232  0.073271  0.073272  0.073351  0.073289  0.073557  0.073498   \n",
      "7270  0.073057  0.073096  0.073097  0.073176  0.073114  0.073381  0.073323   \n",
      "7271  0.074264  0.074303  0.074304  0.074385  0.074322  0.074593  0.317519   \n",
      "7272  0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "7273  0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "7274  0.073422  0.073462  0.073463  0.073542  0.336467  0.073748  0.073690   \n",
      "7275  0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "7276  0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "7277  0.074488  0.074528  0.074529  0.074610  0.074546  0.077286  0.078202   \n",
      "7278  0.073254  0.073293  0.073294  0.073374  0.073311  0.073619  0.073520   \n",
      "7279  0.073048  0.073087  0.073088  0.073167  0.073105  0.073372  0.073314   \n",
      "7280  0.073067  0.073106  0.073107  0.073186  0.073124  0.339140  0.074143   \n",
      "7281  0.073002  0.073041  0.073042  0.073122  0.073059  0.074075  0.073268   \n",
      "7282  0.076517  0.076558  0.076559  0.076642  0.076577  0.076857  0.079682   \n",
      "7283  0.073113  0.073152  0.073153  0.073232  0.073170  0.073437  0.073379   \n",
      "7284  0.074710  0.074750  0.074751  0.074832  0.074769  0.075042  0.074982   \n",
      "7285  0.074483  0.074523  0.074524  0.074605  0.074541  0.074814  0.074754   \n",
      "7286  0.071851  0.071890  0.071890  0.320013  0.071907  0.072170  0.094393   \n",
      "7287  0.079437  0.079479  0.079480  0.079567  0.108872  0.080605  0.241967   \n",
      "7288  0.073057  0.073096  0.073097  0.073176  0.073114  0.073381  0.073323   \n",
      "7289  0.074889  0.074929  0.074930  0.075011  0.077184  0.075221  0.075161   \n",
      "7290  0.075248  0.075288  0.075289  0.075371  0.075306  0.075582  0.136443   \n",
      "7291  0.073057  0.073096  0.073097  0.073176  0.073114  0.073381  0.073323   \n",
      "7292  0.073257  0.073296  0.073297  0.073377  0.073314  0.073582  0.073523   \n",
      "\n",
      "             7         8         9  year_predict  \n",
      "0     0.340144  0.074093  0.073518        1990.0  \n",
      "1     0.086345  0.090200  0.085896        1960.0  \n",
      "2     0.075511  0.338279  0.073574        2000.0  \n",
      "3     0.072481  0.072911  0.072345        1980.0  \n",
      "4     0.073689  0.338946  0.073720        2000.0  \n",
      "5     0.073689  0.338946  0.073720        2000.0  \n",
      "6     0.073843  0.074280  0.073704        1980.0  \n",
      "7     0.073689  0.338946  0.073720        2000.0  \n",
      "8     0.082079  0.335385  0.072945        2000.0  \n",
      "9     0.153647  0.267745  0.072463        2000.0  \n",
      "10    0.073700  0.074307  0.090984        1980.0  \n",
      "11    0.073686  0.074293  0.073716        1980.0  \n",
      "12    0.076160  0.074095  0.073520        1980.0  \n",
      "13    0.074000  0.338832  0.073695        2000.0  \n",
      "14    0.133274  0.076475  0.085999        1950.0  \n",
      "15    0.073689  0.338946  0.073720        2000.0  \n",
      "16    0.088993  0.318795  0.074328        2000.0  \n",
      "17    0.075182  0.074173  0.073597        1980.0  \n",
      "18    0.073689  0.338946  0.073720        2000.0  \n",
      "19    0.073689  0.338946  0.073720        2000.0  \n",
      "20    0.073689  0.338946  0.073720        2000.0  \n",
      "21    0.340144  0.074093  0.073518        1990.0  \n",
      "22    0.082179  0.073795  0.073222        1960.0  \n",
      "23    0.075051  0.074044  0.073469        1970.0  \n",
      "24    0.321383  0.081649  0.074925        1990.0  \n",
      "25    0.073856  0.074464  0.073886        1960.0  \n",
      "26    0.340144  0.074093  0.073518        1990.0  \n",
      "27    0.073689  0.338946  0.073720        2000.0  \n",
      "28    0.340101  0.074210  0.073509        1990.0  \n",
      "29    0.340144  0.074093  0.073518        1990.0  \n",
      "...        ...       ...       ...           ...  \n",
      "7263  0.079679  0.118602  0.079712        1980.0  \n",
      "7264  0.073689  0.338946  0.073720        2000.0  \n",
      "7265  0.340144  0.074093  0.073518        1990.0  \n",
      "7266  0.325415  0.075705  0.075002        1990.0  \n",
      "7267  0.073689  0.338946  0.073720        2000.0  \n",
      "7268  0.340101  0.074210  0.073509        1990.0  \n",
      "7269  0.100664  0.312173  0.073694        2000.0  \n",
      "7270  0.340144  0.074093  0.073518        1990.0  \n",
      "7271  0.074702  0.086875  0.074733        1980.0  \n",
      "7272  0.073689  0.338946  0.073720        2000.0  \n",
      "7273  0.073689  0.338946  0.073720        2000.0  \n",
      "7274  0.073856  0.074464  0.073886        1960.0  \n",
      "7275  0.073689  0.338946  0.073720        2000.0  \n",
      "7276  0.073689  0.338946  0.073720        2000.0  \n",
      "7277  0.075351  0.321500  0.074959        2000.0  \n",
      "7278  0.073686  0.338932  0.073717        2000.0  \n",
      "7279  0.340101  0.074210  0.073509        1990.0  \n",
      "7280  0.073498  0.074103  0.073528        1970.0  \n",
      "7281  0.339889  0.074038  0.073463        1990.0  \n",
      "7282  0.097608  0.285999  0.077000        2000.0  \n",
      "7283  0.075511  0.338279  0.073574        2000.0  \n",
      "7284  0.078522  0.322459  0.075182        2000.0  \n",
      "7285  0.326716  0.076086  0.074954        1990.0  \n",
      "7286  0.080710  0.072870  0.072305        1950.0  \n",
      "7287  0.090091  0.080564  0.079939        1980.0  \n",
      "7288  0.340144  0.074093  0.073518        1990.0  \n",
      "7289  0.321363  0.075951  0.075362        1990.0  \n",
      "7290  0.076119  0.259633  0.075723        2000.0  \n",
      "7291  0.340144  0.074093  0.073518        1990.0  \n",
      "7292  0.073689  0.338946  0.073720        2000.0  \n",
      "\n",
      "[7293 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(combined_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_output.to_csv(\"xgboost_basic_audio.csv\", index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds19_project",
   "language": "python",
   "name": "ds19_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
